{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-18T08:16:05.291832Z",
     "start_time": "2024-12-18T08:16:05.282728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import opencc\n",
    "\n",
    "from utils import read_json, get_path\n",
    "\n",
    "# 创建转换器对象，'t2s' 表示从繁体到简体\n",
    "converter = opencc.OpenCC('t2s.json')\n",
    "\n",
    "# 繁体文本\n",
    "traditional_text = \"我還要感謝我的同學，回憶著我的初中時光，我的腦海總放映著各位同學與我一起學習、進步、相互幫助的畫面，大家總爭著第一，早早地踏入校園，用行動書寫著「一年之計在於晨」。\"\n",
    "# traditional_text = \"回憶著我的初中時光\"\n",
    "\n",
    "# 转换为简体\n",
    "simplified_text = converter.convert(traditional_text)\n",
    "\n",
    "print(simplified_text)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我还要感谢我的同学，回忆著我的初中时光，我的脑海总放映著各位同学与我一起学习、进步、相互帮助的画面，大家总争著第一，早早地踏入校园，用行动书写著「一年之计在于晨」。\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T07:59:35.144250Z",
     "start_time": "2024-12-18T07:59:35.139860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def contains_traditional_chinese(text):\n",
    "    \"\"\"\n",
    "    判断给定的文本中是否包含繁体字\n",
    "    :param text: 输入的文本字符串\n",
    "    :return: 如果包含繁体字，返回 True；否则返回 False\n",
    "    \"\"\"\n",
    "    # 繁体字的 Unicode 范围通常在以下区域\n",
    "    traditional_chinese_range = re.compile(r'[\\u4e00-\\u9fff\\uff00-\\uffef]')\n",
    "    \n",
    "    # 使用 findall 找到所有匹配的字符\n",
    "    matches = traditional_chinese_range.findall(text)\n",
    "    if matches:\n",
    "        print(\"找到的繁体字字符:\", matches)\n",
    "        return True\n",
    "    else:\n",
    "        print(\"没有找到繁体字\")\n",
    "        return False\n",
    "\n",
    "# 测试\n",
    "test_sentence = \"我還要感謝我的同學，回憶著我的初中時光\"\n",
    "print(contains_traditional_chinese(test_sentence))  # 输出 True\n",
    "\n",
    "test_sentence_2 = \"我还要感谢我的同学，回忆着我的初中时光\"\n",
    "print(contains_traditional_chinese(test_sentence_2))  # 输出 False"
   ],
   "id": "485ceb036f917394",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到的繁体字字符: ['我', '還', '要', '感', '謝', '我', '的', '同', '學', '，', '回', '憶', '著', '我', '的', '初', '中', '時', '光']\n",
      "True\n",
      "找到的繁体字字符: ['我', '还', '要', '感', '谢', '我', '的', '同', '学', '，', '回', '忆', '着', '我', '的', '初', '中', '时', '光']\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if text != convert_to_sc(text):\n",
    "\tprint(\"包含简体\")\n",
    "else:\n",
    "    print(\"不包含简体\")"
   ],
   "id": "14b6066376a20e5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T05:48:08.263770Z",
     "start_time": "2024-12-19T05:48:08.231220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils import read_json, get_path\n",
    "json_data = read_json(get_path('lijiang_sents_20241218.json'))\n",
    "print(len(json_data))"
   ],
   "id": "c5cd2c579ea1d9bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10050\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T12:01:09.486645Z",
     "start_time": "2024-12-23T12:01:07.684075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load('Qwen/Qwen2.5-7B-Instruct-MLX', tokenizer_config={\"eos_token\": \"<|im_end|>\"})\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "response = generate(model, tokenizer, prompt=text, verbose=True, top_p=0.8, temp=0.7, repetition_penalty=1.05, max_tokens=512)"
   ],
   "id": "b4cbc13c6dfdf3c8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/essay/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModelNotFoundError",
     "evalue": "Model not found for path or HF repo: Qwen/Qwen2.5-7B-Instruct-MLX.\nPlease make sure you specified the local path or Hugging Face repo id correctly.\nIf you are trying to access a private or gated Hugging Face repo, make sure you are authenticated:\nhttps://huggingface.co/docs/huggingface_hub/en/guides/cli#huggingface-cli-login",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModelNotFoundError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmlx_lm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load, generate\n\u001B[0;32m----> 3\u001B[0m model, tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mQwen/Qwen2.5-7B-Instruct-MLX\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meos_token\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m<|im_end|>\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGive me a short introduction to large language model.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      6\u001B[0m messages \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      7\u001B[0m     {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msystem\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m      8\u001B[0m     {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: prompt}\n\u001B[1;32m      9\u001B[0m ]\n",
      "File \u001B[0;32m/opt/anaconda3/envs/essay/lib/python3.8/site-packages/mlx_lm/utils.py:490\u001B[0m, in \u001B[0;36mload\u001B[0;34m(path_or_hf_repo, tokenizer_config, model_config, adapter_path, lazy)\u001B[0m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\n\u001B[1;32m    463\u001B[0m     path_or_hf_repo: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m    464\u001B[0m     tokenizer_config\u001B[38;5;241m=\u001B[39m{},\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    467\u001B[0m     lazy: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    468\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[nn\u001B[38;5;241m.\u001B[39mModule, TokenizerWrapper]:\n\u001B[1;32m    469\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    470\u001B[0m \u001B[38;5;124;03m    Load the model and tokenizer from a given path or a huggingface repository.\u001B[39;00m\n\u001B[1;32m    471\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    488\u001B[0m \u001B[38;5;124;03m        ValueError: If model class or args class are not found.\u001B[39;00m\n\u001B[1;32m    489\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 490\u001B[0m     model_path \u001B[38;5;241m=\u001B[39m \u001B[43mget_model_path\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_or_hf_repo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    492\u001B[0m     model \u001B[38;5;241m=\u001B[39m load_model(model_path, lazy, model_config)\n\u001B[1;32m    493\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m adapter_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/essay/lib/python3.8/site-packages/mlx_lm/utils.py:94\u001B[0m, in \u001B[0;36mget_model_path\u001B[0;34m(path_or_hf_repo, revision)\u001B[0m\n\u001B[1;32m     79\u001B[0m         model_path \u001B[38;5;241m=\u001B[39m Path(\n\u001B[1;32m     80\u001B[0m             snapshot_download(\n\u001B[1;32m     81\u001B[0m                 repo_id\u001B[38;5;241m=\u001B[39mpath_or_hf_repo,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     91\u001B[0m             )\n\u001B[1;32m     92\u001B[0m         )\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m---> 94\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ModelNotFoundError(\n\u001B[1;32m     95\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel not found for path or HF repo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_hf_repo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     96\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease make sure you specified the local path or Hugging Face\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     97\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m repo id correctly.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mIf you are trying to access a private or\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     98\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m gated Hugging Face repo, make sure you are authenticated:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     99\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/docs/huggingface_hub/en/guides/cli#huggingface-cli-login\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    100\u001B[0m         ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model_path\n",
      "\u001B[0;31mModelNotFoundError\u001B[0m: Model not found for path or HF repo: Qwen/Qwen2.5-7B-Instruct-MLX.\nPlease make sure you specified the local path or Hugging Face repo id correctly.\nIf you are trying to access a private or gated Hugging Face repo, make sure you are authenticated:\nhttps://huggingface.co/docs/huggingface_hub/en/guides/cli#huggingface-cli-login"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
